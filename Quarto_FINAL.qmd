---
title: "Applied Code"
format: html
Names : Anis Bennani, Pierre Commenay, Baptise Abramovici, Emma Widmayer
Software : RStudio
OS : MacOS Sonoma 14.6.1
Version : Version 2024.12.0+467
Date : November 19th 2024
---

## First, we install and load necessary packages

```{r}
# Install packages
install.packages('dplyr') 
install.packages('readr') 
install.packages('marginaleffects') 
install.packages('ggplot2') 
install.packages('xtable')
# Load necessary libraries
library(dplyr) # In order to work with the data frame 
library(readr) # In order to read our csv 
library(marginaleffects) # In order to compute easily the marginal effects of our regression
library(ggplot2) # In order to draw graphics
library(xtable) # In order to write latex directly from r 
```

# 1 : Introduction

The data set used for this study combines databases from 2017 to 2024 of the Annual Social and Economic (ASEC) Supplement from the March 2023 and 2024 editions of the Current Population Survey (CPS). Here, we take household-level databases (containing the FIPS code) and individual-level databases (containing the variables we are interested in). From all these databases, we create our final database, which will allow us to establish the model. Our final database only contains people between 19 and 64, without disability and who are under 150% of the federal poverty level.

In order to spare you from having to run the following code, we will directly include the resulting database : bdd_finale.csv in our folder Output.

## 2 : Data Preparation

### 2.1. Importation of data sets :

```{r}

# Define file paths
base_dir <- "/Users/pierrecommenay/Desktop/QUARTO"  # Path of the folder CPS-ASEC

# You have to modify the file paths above


paths <- list(
  "2017" = c(file.path(base_dir,"CPS-ASEC", "2017", "hhpub17.csv"), file.path(base_dir, "CPS-ASEC", "2017", "pppub17.csv")),
  "2018" = c(file.path(base_dir,"CPS-ASEC", "2018", "hhpub18.csv"), file.path(base_dir,"CPS-ASEC", "2018", "pppub18.csv")),
  "2019" = c(file.path(base_dir,"CPS-ASEC", "2019", "hhpub19.csv"), file.path(base_dir,"CPS-ASEC", "2019", "pppub19.csv")),
  "2020" = c(file.path(base_dir,"CPS-ASEC", "2020", "hhpub20.csv"), file.path(base_dir,"CPS-ASEC", "2020", "pppub20.csv")),
  "2021" = c(file.path(base_dir,"CPS-ASEC", "2021", "hhpub21.csv"), file.path(base_dir,"CPS-ASEC", "2021", "pppub21.csv")),
  "2022" = c(file.path(base_dir,"CPS-ASEC", "2022", "hhpub22.csv"), file.path(base_dir,"CPS-ASEC", "2022", "pppub22.csv")),
  "2023" = c(file.path(base_dir,"CPS-ASEC", "2023", "hhpub23.csv"), file.path(base_dir,"CPS-ASEC", "2023", "pppub23.csv")),
  "2024" = c(file.path(base_dir,"CPS-ASEC", "2024", "hhpub24.csv"), file.path(base_dir,"CPS-ASEC", "2024", "pppub24.csv"))
)

# Function to import data sets
importBase <- function(paths) {
  household <- read_csv(paths[1], show_col_types = FALSE)
  person <- read_csv(paths[2], show_col_types = FALSE)
  return(list(household = household, person = person))
}

# Import data sets
data_list <- lapply(paths, importBase) 

# Define variables of interest
var_pers <- c("H_SEQ", "NOW_COV", "COV", "NOW_PRIV", "A_AGE", "PTOTVAL", "PERLIS", "DSAB_VAL", "NOW_CAID", "A_SEX", "A_HGA", "A_CLSWKR",'GRP','PRDTRACE') # Variables of interest at people scale
var_hh <- c("H_SEQ", "GESTFIPS", "H_YEAR")  # Variables of interest at household scale we need this data set to get the FIPS. We will H_SEQ and H_YEAR for merging with var_pers

# Process household data sets
fusionHousehold <- function(data_list, var_hh) {
  df_list <- lapply(data_list, function(x) {
    x$household %>%
      select(all_of(var_hh)) %>% # Keep only relevant variables for households
      filter(GESTFIPS %in% c(45,51))  # Keep only Virginia (51) and South Carolina (45)
  })
  
  df_household <- bind_rows(df_list)  # Concatenate all data sets
  return(df_household)
}

df_hh <- fusionHousehold(data_list, var_hh)

# Process person data sets
fusionPerson <- function(data_list, var_pers, years) {
  df_list <- mapply(function(data, year) {
    data$person %>%
      rename(H_SEQ = PH_SEQ) %>%  # Rename PH_SEQ to H_SEQ for merging
      select(all_of(var_pers)) %>%
      mutate(H_YEAR = year)  # Add survey year column in order to merge on two keys
  }, data_list, years, SIMPLIFY = FALSE)
  
  df_person <- bind_rows(df_list)  # Concatenate all years
  return(df_person)
}

# Define survey years
survey_years <- c(2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024)

# Run function with survey years
df_pers <- fusionPerson(data_list, var_pers, survey_years)

```

### 2.2. Sample Selection

We restrict the sample to align with Medicaid’s eligibility criteria for adults, and excluding individuals who qualified for Medicare prior to the expansion. Next, we remove individuals with disabilities, as they may have been eligible for Disability Insurance (DI) or other pre-existing public coverage options. Finally, we limit the sample to individuals with incomes below 149% of the Federal Poverty Level (FPL). Due to data constraints, we could not impose a strict 138% FPL threshold; however, we will later discuss the implications of this choice.

```{r}

# Apply sample selection criteria 
df_pers <- df_pers %>%
  filter(A_AGE >= 19, A_AGE <= 64, DSAB_VAL == 0, PERLIS %in% c(1, 2, 3))

```

## 3. Data processing and cleaning

Now, let's rename our columns to have more understandable variable names for the reader and in order to create new variable that we could use in our regressions.

### 3.1. Merging :

```{r}

# Merge person and household data sets
df_merged <- inner_join(df_pers, df_hh, by = c("H_SEQ", "H_YEAR"))

# Remove the DSAB_VAL column (disability income indicator)
df_merged <- df_merged %>% select(-DSAB_VAL)

```

### 3.2. Variable transformations :

```{r}


# Rename columns for better readability
df_merged <- df_merged %>% rename(
  COVERED = NOW_COV,        # Indicator for whether the person currently has health coverage
  COV_LY = COV,             # Indicator for whether the person had health coverage last year
  HOUSEHOLD_ID = H_SEQ,     # Household sequence number
  INCOME = PTOTVAL,         # Total personal income
  PRIVATE = NOW_PRIV,   # Indicator for private health insurance coverage
  AGE = A_AGE,              # Age of the individual
  MEDICAID = NOW_CAID,      # Indicator for Medicaid coverage
  YEAR = H_YEAR,            # Survey year
  SEX = A_SEX,             # Gender indicator (1 = male, 0 = female)
  FIPS = GESTFIPS,      # State indicator (51 = Virginia, 45 = South Carolina)
  WORK = A_CLSWKR,          # Employment classification
  EDUC = A_HGA,             # Educational attainment
  POVERTY_LEVEL = PERLIS,    # Poverty level category
  ETHNICITY = PRDTRACE          #Ethnicity of the individual
)

# All the details on those variables and specification are given in table 1 (see next R window or data appendix)


# Create new binary variables for analysis
df_merged <- df_merged %>%
  mutate(
    VIRGINIA = ifelse(FIPS == 51, 1, 0),  # 1 if the person lives in Virginia, 0 otherwise
    COVERED = ifelse(COVERED == 1, 1, 0),     # 1 if covered by any health insurance, 0 otherwise
    MEDICAID = ifelse(MEDICAID == 1, 1, 0),   # 1 if covered by Medicaid, 0 otherwise
    PRIVATE = ifelse(PRIVATE == 1, 1, 0),    # 1 if covered by private insurance last year, 0 otherwise
    MALE = ifelse(SEX == 1, 1, 0),           # 1 if male, 0 if female
    SUP2019 = ifelse(YEAR >= 2019, 1, 0),     # 1 if the survey year is 2019 or later, 0 otherwise
    HIGHER_BAC = ifelse(EDUC >= 39, 1, 0),         # 1 if education level is master's degree or higher
    TREAT = ifelse(SUP2019== 1 & VIRGINIA == 1, 1, 0), # 1 if in Virginia and post-2019 (treated group)
    POOREST = ifelse(POVERTY_LEVEL ==2,0,1), # 1 if below 124 percent of the federal poverty level
    WHITE = ifelse(ETHNICITY == 1, 1, 0),   # 1 if White
    OTHER_ETHNICITY = ifelse(ETHNICITY >= 2, 1, 0) # 1 if not White
  )

# All the details on transformation are given in Table 2

```

### 3.3. Exporting the data :

```{r}

# Output path 
output_path_bdd <- file.path(base_dir, "Code", "Output", "bdd_finale.csv")

# Export final data set
write_csv(df_merged, output_path_bdd)
```

## 4. Summary statistics and tables for data appendix

### 4.0. Importation of the clean data set :

```{r}

# Import the (clean) data set
df <- read_csv(output_path_bdd)
```

### 4.1. Descriptive statistics

This window allows us to compute all the descriptive statistics for our database. The results have been reported in a LaTeX table, which you will find in the final paper submission. We compute, for each variable the mean, the minimum, maximum and the standard deviation.

```{r}

####################################################################################################################################
#Descriptive Statistics


#In Virginia
#Number of individuals for each Year in Virginia

treat_2017 = sum(df$YEAR == 2017 & df$VIRGINIA == 1)
treat_2018 = sum(df$YEAR == 2018 & df$VIRGINIA == 1)
treat_2019 = sum(df$YEAR == 2019 & df$VIRGINIA == 1)
treat_2020 = sum(df$YEAR == 2020 & df$VIRGINIA == 1)
treat_2022 = sum(df$YEAR == 2022 & df$VIRGINIA == 1)
treat_2023 = sum(df$YEAR == 2023 & df$VIRGINIA == 1)
treat_2024 = sum(df$YEAR == 2024 & df$VIRGINIA == 1)
treat_2021 = sum(df$YEAR == 2021 & df$VIRGINIA == 1)

#Creation of a list for post-treatment and pre-treatment years

treat_by_year_before <- list(
  "2017" = treat_2017,
  "2018" = treat_2018)

treat_by_year_after <- list(
  "2019" = treat_2019,
  "2020" = treat_2020,
  "2021" = treat_2021,
  "2022" = treat_2022,
  "2023" = treat_2023,
  "2024" = treat_2024)

#Creation of two functions that compute the minimum, maximum, standard deviation and mean values for post-treatment and pre-treatment periods in Virginia for a specific variable

# First for the pre-treatment period
calculate_cov_virginia_before <- function(data, var_name, treat_values) {
  years <- 2017:2018
  
  cov_values <- sapply(years, function(year) {
    sum(data$YEAR == year & data$VIRGINIA == 1 & data[[var_name]] == 1) / treat_values[[as.character(year)]]
  })
  # Mean
  mean_cov <- mean(cov_values, na.rm = TRUE)
  # Minimum
  min_cov <- min(cov_values, na.rm = TRUE)
  # Maximum
  max_cov <- max(cov_values, na.rm = TRUE)
  # Standard deviation
  sd_cov <-  sd(cov_values, na.rm = TRUE)
  return(list(mean = mean_cov, min = min_cov, max = max_cov, sd = sd_cov))
}

#Secondly for the post-treatment period
calculate_cov_virginia_after <- function(data, var_name, treat_values) {
  years <- 2019:2024
  
  cov_values <- sapply(years, function(year) {
    sum(data$YEAR == year & data$VIRGINIA == 1 & data[[var_name]] == 1) / treat_values[[as.character(year)]]
  })
  # Mean
  mean_cov <- mean(cov_values, na.rm = TRUE)
  # Minimum
  min_cov <- min(cov_values, na.rm = TRUE)
  # Maximum
  max_cov <- max(cov_values, na.rm = TRUE)
  # Standard deviation
  sd_cov <-  sd(cov_values, na.rm = TRUE)
  return(list(mean = mean_cov, min = min_cov, max = max_cov, sd = sd_cov))
}

#Number of people each year in each state
mean = sum(treat_2017, treat_2018)/ 2
mean
min(treat_2017, treat_2018)
max(treat_2017, treat_2018)
sd(c(treat_2017, treat_2018))

mean = sum(treat_2019, treat_2020, treat_2022,
           treat_2023, treat_2024, treat_2021 )/ 6
mean
min(treat_2019, treat_2020, treat_2022,
    treat_2023, treat_2024, treat_2021 )
max(treat_2019, treat_2020, treat_2022,
    treat_2023, treat_2024, treat_2021 )
sd(c(treat_2019, treat_2020, treat_2022,
    treat_2023, treat_2024, treat_2021 ))

#Computation of the descriptive statistics for each variable, post and pre-treatment

calculate_cov_virginia_before(df, "COVERED", treat_by_year_before)
calculate_cov_virginia_before(df, "MALE", treat_by_year_before)
calculate_cov_virginia_before(df, "HIGHER_BAC", treat_by_year_before)
calculate_cov_virginia_before(df, "PRIVATE", treat_by_year_before)
calculate_cov_virginia_before(df, "POOREST", treat_by_year_before)
calculate_cov_virginia_before(df, "WHITE", treat_by_year_before)
calculate_cov_virginia_before(df, "OTHER_ETHNICITY", treat_by_year_before)

calculate_cov_virginia_after(df, "COVERED", treat_by_year_after)
calculate_cov_virginia_after(df, "MALE", treat_by_year_after)
calculate_cov_virginia_after(df, "HIGHER_BAC", treat_by_year_after)
calculate_cov_virginia_after(df, "PRIVATE", treat_by_year_after)
calculate_cov_virginia_after(df, "POOREST", treat_by_year_after)
calculate_cov_virginia_after(df, "WHITE", treat_by_year_after)
calculate_cov_virginia_after(df, "OTHER_ETHNICITY", treat_by_year_after)


#Income's descriptive statistics

inc1 = mean(df$INCOME[df$YEAR == 2017 & df$VIRGINIA == 1], na.rm = TRUE)
inc2 = mean(df$INCOME[df$YEAR == 2018 & df$VIRGINIA == 1], na.rm = TRUE)
inc3 = mean(df$INCOME[df$YEAR == 2019 & df$VIRGINIA == 1], na.rm = TRUE)
inc4 = mean(df$INCOME[df$YEAR == 2020 & df$VIRGINIA == 1], na.rm = TRUE)
inc5 = mean(df$INCOME[df$YEAR == 2021 & df$VIRGINIA == 1], na.rm = TRUE)
inc6 = mean(df$INCOME[df$YEAR == 2022 & df$VIRGINIA == 1], na.rm = TRUE)
inc7 = mean(df$INCOME[df$YEAR == 2023 & df$VIRGINIA == 1], na.rm = TRUE)
inc8 = mean(df$INCOME[df$YEAR == 2024 & df$VIRGINIA == 1], na.rm = TRUE)

sum(inc1, inc2)/2
min(inc1, inc2)
max(inc1, inc2)
sd(c(inc1, inc2))

sum(inc3,inc4,inc5,inc6,inc7,inc8)/6
min(inc3,inc4,inc5,inc6,inc7,inc8)
max(inc3,inc4,inc5,inc6,inc7,inc8)
sd(c(inc3,inc4,inc5,inc6,inc7,inc8))


###############################################################################################
#In South Carolina
#Again we repeat the procedure for South Carolina
#Number of individuals for each Year in South Carolina

no_treat_2017 = sum(df$YEAR == 2017 & df$VIRGINIA == 0)
no_treat_2018 = sum(df$YEAR == 2018 & df$VIRGINIA == 0)
no_treat_2019 = sum(df$YEAR == 2019 & df$VIRGINIA == 0)
no_treat_2020 = sum(df$YEAR == 2020 & df$VIRGINIA == 0)
no_treat_2022 = sum(df$YEAR == 2022 & df$VIRGINIA == 0)
no_treat_2023 = sum(df$YEAR == 2023 & df$VIRGINIA == 0)
no_treat_2024 = sum(df$YEAR == 2024 & df$VIRGINIA == 0)
no_treat_2021 = sum(df$YEAR == 2021 & df$VIRGINIA == 0)

#Creation of a list for post-treatment and pre-treatment years

treat_by_year_before_treat <- list(
  "2017" = no_treat_2017,
  "2018" = no_treat_2018)

treat_by_year_after_treat <- list(
  "2019" = no_treat_2019,
  "2020" = no_treat_2020,
  "2021" = no_treat_2021,
  "2022" = no_treat_2022,
  "2023" = no_treat_2023,
  "2024" = no_treat_2024)

#Creation of two functions that compute the minimum, maximum, standard deviantion and mean values for post-treatment and pre-treatment periods in Virginia for a specific variable

#First for pre-treatment periods
calculate_cov_south_before <- function(data, var_name, treat_values) {
  years <- 2017:2018

  cov_values <- sapply(years, function(year) {
    sum(data$YEAR == year & data$VIRGINIA == 0 & data[[var_name]] == 1) / treat_values[[as.character(year)]]
  })
  # Mean
  mean_cov <- mean(cov_values, na.rm = TRUE)
  # Minimum
  min_cov <- min(cov_values, na.rm = TRUE)
  # Maximum
  max_cov <- max(cov_values, na.rm = TRUE)
  # Standard deviation
  sd_cov <-  sd(cov_values, na.rm = TRUE)
  return(list(mean = mean_cov, min = min_cov, max = max_cov, sd = sd_cov))
}


#Secondly post-treatments periods
calculate_cov_south_after <- function(data, var_name, treat_values) {
  years <- 2019:2024
  
  cov_values <- sapply(years, function(year) {
    sum(data$YEAR == year & data$VIRGINIA == 0 & data[[var_name]] == 1) / treat_values[[as.character(year)]]
  })
  # Mean
  mean_cov <- mean(cov_values, na.rm = TRUE)
  # Minimum
  min_cov <- min(cov_values, na.rm = TRUE)
  # Maximum
  max_cov <- max(cov_values, na.rm = TRUE)
  # Standard deviation
  sd_cov <-  sd(cov_values, na.rm = TRUE)
  return(list(mean = mean_cov, min = min_cov, max = max_cov, sd = sd_cov))
}


#Number of people each year in each state
mean = sum(no_treat_2017, no_treat_2018)/ 2
mean
min(no_treat_2017, no_treat_2018)
max(no_treat_2017, no_treat_2018)
sd(c(no_treat_2017, no_treat_2018))

mean = sum(no_treat_2019, no_treat_2020, no_treat_2022,
           no_treat_2023, no_treat_2024, no_treat_2021 )/ 6
mean
min(no_treat_2019, no_treat_2020, no_treat_2022,
    no_treat_2023, no_treat_2024, no_treat_2021 )
max(no_treat_2019, no_treat_2020, no_treat_2022,
    no_treat_2023, no_treat_2024, no_treat_2021 )
sd(c(no_treat_2019, no_treat_2020, no_treat_2022,
    no_treat_2023, no_treat_2024, no_treat_2021))

#Computation of the descriptive statistics for each variable, post and pre-treatment

calculate_cov_south_before(df, "COVERED", treat_by_year_before)
calculate_cov_south_before(df, "MALE", treat_by_year_before)
calculate_cov_south_before(df, "HIGHER_BAC", treat_by_year_before)
calculate_cov_south_before(df, "PRIVATE", treat_by_year_before)
calculate_cov_south_before(df, "POOREST", treat_by_year_before)
calculate_cov_south_before(df, "WHITE", treat_by_year_before)
calculate_cov_south_before(df, "OTHER_ETHNICITY", treat_by_year_before)

calculate_cov_south_after(df, "COVERED", treat_by_year_after)
calculate_cov_south_after(df, "MALE", treat_by_year_after)
calculate_cov_south_after(df, "HIGHER_BAC", treat_by_year_after)
calculate_cov_south_after(df, "PRIVATE", treat_by_year_after)
calculate_cov_south_after(df, "POOREST", treat_by_year_after)
calculate_cov_south_after(df, "WHITE", treat_by_year_after)
calculate_cov_south_after(df, "OTHER_ETHNICITY", treat_by_year_after)

#Income's descriptive statistics

inc12 = mean(df$INCOME[df$YEAR == 2017 & df$VIRGINIA == 0], na.rm = TRUE)
inc22 = mean(df$INCOME[df$YEAR == 2018 & df$VIRGINIA == 0], na.rm = TRUE)

inc32 = mean(df$INCOME[df$YEAR == 2019 & df$VIRGINIA == 0], na.rm = TRUE)
inc42 = mean(df$INCOME[df$YEAR == 2020 & df$VIRGINIA == 0], na.rm = TRUE)
inc52 = mean(df$INCOME[df$YEAR == 2021 & df$VIRGINIA == 0], na.rm = TRUE)
inc62 = mean(df$INCOME[df$YEAR == 2022 & df$VIRGINIA == 0], na.rm = TRUE)
inc72 = mean(df$INCOME[df$YEAR == 2023 & df$VIRGINIA == 0], na.rm = TRUE)
inc82 = mean(df$INCOME[df$YEAR == 2024 & df$VIRGINIA == 0], na.rm = TRUE)

sum(inc12, inc22)/2
min(inc12, inc22)
max(inc12, inc22)
sd(c(inc12, inc22))

sum(inc32,inc42,inc52,inc62,inc72,inc82)/6
min(inc32,inc42,inc52,inc62,inc72,inc82)
max(inc32,inc42,inc52,inc62,inc72,inc82)
sd(c(inc32,inc42,inc52,inc62,inc72,inc82))


```

### 4.2. Tables for data appendix

#### Table 1 : Specification and selection of variables

```{r}

var_pers <- c("HOUSEHOLD_ID", "COVERED", "COV_LY", "PRIVATE", "AGE", "INCOME", "POVERTY_LEVEL", "MEDICAID", "YEAR", "SEX", "EDUC", "WORK", "FIPS", "ETHNICITY")

table_1 <- data.frame(
  Variable = var_pers,
  Label = c("Household sequence number", 
            "Currently covered by health insurance", 
            "Any health insurance coverage last year", 
            "Covered by private insurance", 
            "Age of the individual", 
            "Total personal income", 
            "Poverty level category", 
            "Current Medicaid coverage", 
            "Survey year", 
            "Gender Indicator", 
            "Educational attainment", 
            "Employment classification", 
            "FIPS State Indicator", 
            "Ethnicity of the individual"),
  Selection = c("", "", "", "", "19-64", "", "1,2 and 3 (individual between 0 and 150% of the federal poverty level)", "", "2017-2024", "", "", "", "45 (South Carolina) or 51 (Virginia)", "")
)

# Export to LaTeX

output_path_table1 <- file.path(base_dir, "Code", "Output", "Table_A1_Specification_and_Selection_of_CPS-ASEC_variables.tex")


# Generating Latex code
latex_code_1 <- print(xtable(table_1, type = "latex"), include.rownames = FALSE)

# Exporting results
writeLines(latex_code_1, output_path_table1)
```

#### Table 2 : Transformation of variables for analysis

```{r}

# Creation of Table 2: Transformation of variables for analysis
table_2 <- data.frame(
  Variable = c("COVERED", "", "MEDICAID", "", "PRIVATE", "", "MALE", "", "SUP2019", "", "HIGHER_BAC", "", "TREAT", "", "POOREST", "", "WHITE", "", "OTHER_ETHNICITY", ""),
  Label = c("Currently covered by health insurance", "", 
            "Currently covered by Medicaid", "", 
            "Covered by private insurance", "", 
            "Indicator for gender", "", 
            "Indicator for before/after expansion", "", 
            "Indicator if the individual has at least a Highschool diploma", "", 
            "Indicator for treatment", "", 
            "Indicator for poverty level", "", 
            "Indicator for White ethnicity", "", 
            "Indicator for non-White ethnicity", ""),
  Value = c("Not covered", "Covered", 
            "Not covered", "Covered", 
            "Not covered", "Covered", 
            "Female", "Male", 
            "Before expansion", "After expansion", 
            "Lower than highschool diploma", "At least high school diploma", 
            "Control or not treated yet", "Treated", 
            "Between 125% and 150% of FPL", "Below 125% of FPL", 
            "Not White", "White", 
            "White", "Other Ethnicities"),
  Code = c(0, 1, 
           0, 1, 
           0, 1, 
           0, 1, 
           0, 1, 
           0, 1, 
           0, 1, 
           0, 1, 
           0, 1,
           0, 1)
)

# Define the output file path
output_path_table2 <- file.path(base_dir, "Code", "Output", "Table_A2_Transformation_of_CPS-ASEC_variables.tex")

# Generate LaTeX code from the table
latex_code_2 <- print(xtable(table_2, type = "latex"), include.rownames = FALSE)

# Write the LaTeX table to a file
writeLines(latex_code_2, output_path_table2)

```

## 5. Analysis

### 5.0. Importation of the clean dataset

```{r}

# Import the (clean) data set
df <- read_csv(output_path_bdd)
```

### 5.1. Parallel trends

Here, we plot the graph of the coverage percentage by year to assess whether the treatment and control groups evolve in the same way before treatment and whether the treatment induces a change in the trend of the control group.

#### Plots for parallel trends

Here, we clearly observe a change in the evolution of the percentage of covered individuals post-treatment. Additionally, the two groups appear to evolve in the same way before the treatment, which strengthens our assumption of parallel trends validity requirement for the subsequent modeling of our difference-in-differences approach.

```{r}

# Calculate the proportion of covered individuals by state and year
cov_trends <- df %>%
  group_by(YEAR, VIRGINIA) %>%
  summarise(cov_rate = mean(COVERED == 1, na.rm = TRUE), .groups = "drop")

# Plot the graph, we use the package ggplot2 to create the graph
ggplot(cov_trends, aes(x = YEAR, y = cov_rate, color = as.factor(VIRGINIA))) +
  geom_line() +
  geom_vline(xintercept = 2019, linetype = "dashed", color = "red") +  # Vertical line for the treatment year
  labs(x = "Year", y = "Proportion of individuals with any type of coverage", color = "State") +

  scale_color_manual(values = c("0" = "blue", "1" = "red"), 
                     labels = c("0" = "South Carolina", "1" = "Virginia"),  
                     name = "States") +  
  theme_minimal()



```

The two groups appear to evolve in the same way before the treatment, which strengthens our assumption of parallel trends validity requirement for the subsequent modeling of our difference-in-differences approach.

```{r}

# Calculate the proportion of individuals with private insurance by state and year
priv_trends <- df %>%
  group_by(YEAR, VIRGINIA) %>%
  summarise(priv_rate = mean(PRIVATE == 1, na.rm = TRUE), .groups = "drop")

# Plot the graph
# Plot the graph, we use the package ggplot2 to create the graph
ggplot(priv_trends, aes(x = YEAR, y = priv_rate, color = as.factor(VIRGINIA))) +
  geom_line() +
  geom_vline(xintercept = 2019, linetype = "dashed", color = "red") +  # Vertical line for the treatment year
  labs(x = "Year", y = "Proportion of individual with private insurance", color = "State") +

  scale_color_manual(values = c("0" = "blue", "1" = "red"),  
                     labels = c("0" = "South Carolina", "1" = "Virginia"),  
                     name = "States") +  
  theme_minimal()
```

### 5.1. Difference-in-difference estimation without covariate

Difference in difference without covariate. The results have been reported in a LaTeX table, which you will find in the final paper submission.

#### DiD estimation on Covered people

```{r}

#To proceed, we use the function glm, doing a regression using a probit of COVERED on our explained variables
probit_model_COVERED <- glm(COVERED ~ SUP2019 + VIRGINIA + TREAT, data=df, family = binomial(link="probit"))
summary(probit_model_COVERED)

#This command allows to compute the average marginal effects of being in Virginia in the post period treatment on the private insurance's rate
avg_slopes(probit_model_COVERED)

```

With the first result, we can only observe the sign of the variation and its statistical significance.The result confirms our hypothesis that being in Virginia after 2019 has a positive effect on the probability of having health coverage for individuals aged 19 to 65 with an income below 200% of the Federal Poverty Level. Moreover, the test associated with the regression, where the null hypothesis is tested against the alternative, is significant at the 1% level. This allows us to confidently reject the null hypothesis. The *marginaleffects* package helps us interpret the economic impact. According to our model, for a person aged 19 to 65 with an income below 150% of the Federal Poverty Level, being in Virginia after 2019 increases the probability of having health coverage by an average of 9.4 percentage points.

#### DiD estimation on Private Insured people

```{r}

#DiD estimation on PRIVATE

#To proceed, we use the function glm, doing a regression using a probit of PRIVATE on our explained variables
probit_model_PRIVATE <- glm(PRIVATE ~ SUP2019 + VIRGINIA + TREAT, data=df, family = binomial(link="probit"))
summary(probit_model_PRIVATE)

#This command allows to compute the average marginal effects of being in Virginia in the post period treatment on the coverage's rate
avg_slopes(probit_model_PRIVATE)
```

The probit regression results indicate a modest crowd-out effect of private insurance following Virginia’s 2019 Medicaid expansion. The negative and statistically significant treatment coefficient (p-value = 0.016) suggests a decline in the probability of having a private coverage among the treated population. Average marginal effects estimate a 7.45 percentage point reduction in private insurance enrollment, consistent with a substitution effect where some low-income adults shifted from private plans to Medicaid. However, this effect remains limited compared to the overall increase in health coverage, aligning with prior research (Gruber & Sommers, 2019; Lyu & Wehby, 2022) showing that Medicaid expansion primarily reduced the uninsured rate rather than displacing private insurance.

### 5.2. Placebo test

A placebo test in the context of a multi-period difference-in-differences model with two states (South Carolina and Virginia) allows us to verify whether the control and treatment groups exhibit similar characteristics during the pre-treatment period.

The results have been reported in a LaTeX table, which you will find in the final paper submission.

```{r}

# treatment_year
year_treatment <- 2019
# Converting Year in a numeric variable
df <- df %>%
mutate(YEAR = as.numeric(YEAR),
year = YEAR)
# We create temporal variables taking values t-2019 where 2019 is the treatment year
df <- df %>%
mutate(
time_to_treat = year - year_treatment, # Number of years pre/post treatment
treat_time = as.factor(time_to_treat)) # Categorical Variable 
df <- df %>%
mutate(
Tm2 = ifelse(time_to_treat == -2, 1, 0),
Tm1 = ifelse(time_to_treat == -1, 1, 0),
)
#We create variables taking the value Tm1 or Tm2 if and only if the observation is in Virginia
df$TREAT_Tm2 = df$Tm2 * df$VIRGINIA
df$TREAT_Tm1 = df$Tm1 * df$VIRGINIA

#A placebo test, which is a probit model, is useful to verify the parallel trends assumption
#between the control and treatment groups. If the estimators are not significants, we can assume parallel trends
probit_placebo_test_on_covered = glm(COVERED~ SUP2019 + TREAT_Tm2 + TREAT_Tm1
, data=df, family = binomial(link="probit") )
summary(probit_placebo_test_on_covered)


probit_placebo_test_on_private = glm(PRIVATE~ SUP2019 + TREAT_Tm2 + TREAT_Tm1
, data=df, family = binomial(link="probit") )
summary(probit_placebo_test_on_private)
```

We have pre-treatment covariates that significantly impact the proportion of covered individuals and the proportion of privately insured individuals. This suggests a violation of the parallel pre-trend assumption. Therefore, we will select covariates that may explain the differences in group characteristics responsible for this violation.

### 5.3. Balancing tests

Given this potential violation of the parallel pre-trend assumption, it is crucial to further investigate the differences between the treatment and control groups. To do so, we first estimate our Difference-in-Differences (DiD) model without covariate. However, the literature suggests that certain variables may be correlated with the treatment group, potentially influencing our results. To assess whether these variables were already imbalanced between the two states before the treatment, we conduct a balancing test. This allows us to identify any pre-treatment differences in key covariates.

We use two methods to assess the balance between the treatment and control groups. First, we apply the difference-in-means test, which provides a simple and intuitive way to identify which variables are unbalanced between the groups and to estimate the magnitude of this imbalance. Next, we conduct regressions to quantitatively evaluate the unbalance, allowing us to better understand the size and significance of the impact of each variable. The complementarity of both approaches strengthens our results: the difference-in-means test gives us a quick, broad assessment, while the regression analysis allows for a deeper, more precise examination of the relationships between variables and the treatment group.

We compute this test using two different methods, and the results are presented in a LaTeX table included in the final paper submission.

#### Testing difference between means before 2019 :

Assessing the balancing assumption by comparing the difference in means (or in proportions) for key potential cofounders The results have been reported in a LaTeX table, which you will find in the final paper submission.

```{r}


# BALANCING TEST FOR CONTINUOUS VARIABLES (Student t-test)

# NB : We first assume the normality of the distribution of our variables among each group (and run a Shapiro-Wilk test to account for it)
# NB : We also assume the equality of the variances for each variable within each group (and run a Fisher test to account for it)

# NB : We will still run the t-test eventhough the two assumptions don't seem reasonable
# NB : We will keep that limit in mind when appreciating the validity of our design 



# BALANCING TEST FOR AGE

# We filter our data to extract age for each state before the expansion
age_virginia <- df$AGE[df$VIRGINIA == 1 & df$SUP2019 == 0]
age_south_carolina <- df$AGE[df$VIRGINIA == 0 & df$SUP2019 == 0]

# We check for summary statistics within each group
summary(age_virginia)
summary(age_south_carolina)

# We check for the normality assumption of the distribution of our variables (Shapiro-Wilk test (1965) for a sample size < 5000)
shapiro.test(age_virginia)  
shapiro.test(age_south_carolina) 

# We check for the equality of the variances of our variables within each group (Fisher test)
var.test(age_virginia, age_south_carolina)  

# Running the Student t-test
t_test_result <- t.test(age_virginia, age_south_carolina, var.equal = FALSE)
print(t_test_result) # p-value = 0.03488


# BALANCING TEST FOR INCOME

# We filter our data to extract INCOME for each state before the expansion
income_virginia <- df$INCOME[df$VIRGINIA == 1 & df$SUP2019 == 0]
income_south_carolina <- df$INCOME[df$VIRGINIA == 0 & df$SUP2019 == 0]

# We check for summary statistics within each group
summary(income_virginia)
summary(income_south_carolina)

# We check for the normality assumption of the distribution of our variables (Shapiro-Wilk test (1965) for a sample size < 5000)
shapiro.test(income_virginia)  
shapiro.test(income_south_carolina) 

# We check for the equality of the variances of our variables within each group (Fisher test)
var.test(income_virginia, income_south_carolina)  

# Running the Student t-test
t_test_result <- t.test(income_virginia, income_south_carolina, var.equal = FALSE)
print(t_test_result) # p-value = 0.1951



# FOR BINARY VARIABLES (Chi-squared test)


# BALANCING TEST FOR MALE
df_balancing <- df[df$SUP2019 == 0, ] #Create the dataframe for balancing test

# We create a contingency table of MALE by VIRGINIA
male_contingency_table <- table(df_balancing$MALE, df_balancing$VIRGINIA)
print(male_contingency_table)

# Running the Chi-squared test
chi2_test_result <- chisq.test(male_contingency_table)
print(chi2_test_result) # p-value = 0.2653


# BALANCING TEST FOR WHITE

# We create a contingency table of WHITE by VIRGINIA
white_contingency_table <- table(df_balancing$WHITE, df_balancing$VIRGINIA)
print(white_contingency_table)

# Running the Chi-squared test
chi2_test_result <- chisq.test(white_contingency_table)
print(chi2_test_result) # p-value = 0.0007209


# BALANCING TEST FOR OTHER_ETHNICITY

# We create a contingency table of OTHER_ETHNICITY by VIRGINIA
other_ethnicity_contingency_table <- table(df_balancing$OTHER_ETHNICITY, df_balancing$VIRGINIA)
print(other_ethnicity_contingency_table)

# Running the Chi-squared test
chi2_test_result <- chisq.test(other_ethnicity_contingency_table)
print(chi2_test_result) # p-value = 0.0007209
```

#### Regression of each covariate on Virginia before 2019

We then, conduct a balancing test by regressing a characteristic variable on the treatment group during the pre-treatment period. We conclude that if being in the treatment group leads to a statistically significant change in the variable on average, then the groups are not balanced between the two states.

```{r}


df_balancing <- df[df$SUP2019 == 0, ] #Create the dataframe for balancing test

# For MASTER
balancing_educ2 <- glm( HIGHER_BAC  ~ VIRGINIA, data = df_balancing, family = binomial(link="probit"))
summary(balancing_educ2)

# For AGE
balancing_age <- lm(AGE ~  VIRGINIA  , data = df_balancing)
summary(balancing_age)

# For MALE
balancing_male <- glm(MALE ~  VIRGINIA , data = df_balancing, family = binomial(link="probit"))
summary(balancing_male)

# For POOREST
balancing_poorest <- glm(POOREST ~ VIRGINIA, data = df_balancing, family = binomial(link="probit"))
summary(balancing_poorest)

# For WHITE
balancing_white <- glm(WHITE ~  VIRGINIA , data = df_balancing, family = binomial(link="probit"))
summary(balancing_white)

# For OTHER_ETHNICICITY
balancing_other_ethnicity <- glm(OTHER_ETHNICITY ~   VIRGINIA, data = df_balancing, family = binomial(link="probit"))
summary(balancing_other_ethnicity)


```

We finally conclude from these two methods that it might be wise to control for characteristic variables that are significantly different between the two groups.

### 5.4. Difference-in-difference with covariates

We perform the regressions while controlling for the covariates identified in the previous section that were found to be unbalanced between the groups.

The results have been reported in a LaTeX table, which you will find in the final paper submission.

#### DiD estimation on Covered people

```{r}

#To proceed, we use the function glm, doing a regression using a probit of COVERED on our explained variables
probit_model_COVERED_with_covariates <- glm(COVERED ~ AGE + WHITE + VIRGINIA + SUP2019 + TREAT, data=df, family = binomial(link="probit"))
summary(probit_model_COVERED_with_covariates)

#This command allows to compute the average marginal effects of being in Virginia in the post period treatment on the private insurance's rate
avg_slopes(probit_model_COVERED_with_covariates)
```

#### DiD estimation on Private Insured people

```{r}

#To proceed, we use the function glm, doing a regression using a probit of PRIVATE on our explained variables
probit_model_PRIVATE_with_covariates <- glm(PRIVATE ~ AGE + WHITE + VIRGINIA + SUP2019 + TREAT, data=df, family = binomial(link="probit"))
summary(probit_model_PRIVATE_with_covariates)

#This command allows to compute the average marginal effects of being in Virginia in the post period treatment on the coverage's rate
avg_slopes(probit_model_PRIVATE_with_covariates)
```
